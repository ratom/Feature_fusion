{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-03T03:05:47.209198Z",
     "iopub.status.busy": "2023-11-03T03:05:47.208185Z",
     "iopub.status.idle": "2023-11-03T03:06:00.774143Z",
     "shell.execute_reply": "2023-11-03T03:06:00.773109Z",
     "shell.execute_reply.started": "2023-11-03T03:05:47.209164Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import ViTFeatureExtractor, ViTModel, ViTConfig, AutoConfig\n",
    "\n",
    "from PIL import Image\n",
    "#from torchsummary import summary\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T03:06:01.282003Z",
     "iopub.status.busy": "2023-11-03T03:06:01.281405Z",
     "iopub.status.idle": "2023-11-03T03:06:01.470987Z",
     "shell.execute_reply": "2023-11-03T03:06:01.470175Z",
     "shell.execute_reply.started": "2023-11-03T03:06:01.281976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Training data\n",
    "df = pd.read_excel('/kaggle/input/cowimages/cowimagelow/train.xlsx')\n",
    "df[\"file_path\"] = '/kaggle/input/cowimages/cowimagelow/train/'+df['file']\n",
    "df[\"label\"] = df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T03:06:01.472891Z",
     "iopub.status.busy": "2023-11-03T03:06:01.472592Z",
     "iopub.status.idle": "2023-11-03T03:06:01.480579Z",
     "shell.execute_reply": "2023-11-03T03:06:01.479618Z",
     "shell.execute_reply.started": "2023-11-03T03:06:01.472866Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function to add data/model in to GPU (cuda)\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "def to_device(data, device):\n",
    "    # if data is list or tuple, move each of them to device\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device) -> None:\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            # yield only execuate when the function is called\n",
    "            yield to_device(b, self. device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T03:06:02.073675Z",
     "iopub.status.busy": "2023-11-03T03:06:02.073307Z",
     "iopub.status.idle": "2023-11-03T03:06:02.382150Z",
     "shell.execute_reply": "2023-11-03T03:06:02.381208Z",
     "shell.execute_reply.started": "2023-11-03T03:06:02.073648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77816bdf573b4e70b1b1fb554d4307ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training dataset\n",
    "class cattleDataset(Dataset):\n",
    "    def __init__(self, dataframe, trans_transform=None, res_transform=None):\n",
    "        self.labels = dataframe[\"label\"]\n",
    "        self.images = dataframe[\"file_path\"]\n",
    "        self.trans_transform = trans_transform\n",
    "        self.res_transform = res_transform\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        image_trans = self.trans_transform(np.array(image), return_tensors='pt')\n",
    "        image_trans = image_trans['pixel_values'].squeeze()\n",
    "\n",
    "        image_res = self.res_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image_trans, image_res, label\n",
    "\n",
    "trans_transform = ViTFeatureExtractor.from_pretrained('google/vit-large-patch16-224')\n",
    "res_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "train_ds = cattleDataset(df, trans_transform=trans_transform, res_transform=res_transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T03:06:13.772351Z",
     "iopub.status.busy": "2023-11-03T03:06:13.771513Z",
     "iopub.status.idle": "2023-11-03T03:06:13.835935Z",
     "shell.execute_reply": "2023-11-03T03:06:13.834967Z",
     "shell.execute_reply.started": "2023-11-03T03:06:13.772317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Training data\n",
    "df_test = pd.read_excel(\"/kaggle/input/cowimages/cowimagelow/test.xlsx\")\n",
    "df_test[\"file_path\"] = '/kaggle/input/cowimages/cowimagelow/test/'+df_test[\"file\"]# Create image path\n",
    "df_test[\"label\"] = df_test[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T03:06:17.204604Z",
     "iopub.status.busy": "2023-11-03T03:06:17.203770Z",
     "iopub.status.idle": "2023-11-03T03:06:17.209496Z",
     "shell.execute_reply": "2023-11-03T03:06:17.208569Z",
     "shell.execute_reply.started": "2023-11-03T03:06:17.204572Z"
    }
   },
   "outputs": [],
   "source": [
    "test_ds = cattleDataset(df_test, trans_transform=trans_transform, res_transform=res_transform)\n",
    "val_dl = DataLoader(test_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T03:06:20.463825Z",
     "iopub.status.busy": "2023-11-03T03:06:20.463432Z",
     "iopub.status.idle": "2023-11-03T03:06:24.169358Z",
     "shell.execute_reply": "2023-11-03T03:06:24.168453Z",
     "shell.execute_reply.started": "2023-11-03T03:06:20.463794Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Modify the model - ResNet\n",
    "model_Res = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=False)\n",
    "\n",
    "# Remove the last layer of the model Res\n",
    "layers_Res = list(model_Res.children())\n",
    "model_Res = nn.Sequential(*layers_Res[:-1])\n",
    "\n",
    "# Set the top layers to be not trainable\n",
    "count = 0\n",
    "for child in model_Res.children():\n",
    "    count += 1\n",
    "    if count < 8:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "# Modify the model - ViT model\n",
    "\n",
    "from transformers import ViTModel, ViTConfig\n",
    "# Load the pre-trained ViT model\n",
    "config = ViTConfig()\n",
    "model_trans = ViTModel(config)\n",
    "count = 0\n",
    "for child in model_trans.children():\n",
    "    count += 1\n",
    "    if count < 4:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "layers_trans = list(model_trans.children()) # Get all the layers from the Transformer model\n",
    "model_trans_top = nn.Sequential(*layers_trans[:-2]) # Remove the normalization layer and pooler layer\n",
    "trans_layer_norm = list(model_trans.children())[2] # Get the normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T03:06:27.046176Z",
     "iopub.status.busy": "2023-11-03T03:06:27.045340Z",
     "iopub.status.idle": "2023-11-03T03:06:27.056088Z",
     "shell.execute_reply": "2023-11-03T03:06:27.055064Z",
     "shell.execute_reply.started": "2023-11-03T03:06:27.046143Z"
    }
   },
   "outputs": [],
   "source": [
    "class model_final(nn.Module):\n",
    "    def __init__(self, model_trans_top, model_Res, dp_rate = 0.3):\n",
    "        super().__init__()\n",
    "        # All the trans model layers\n",
    "        self.model_trans_top = model_trans_top\n",
    "        #self.trans_layer_norm = trans_layer_norm\n",
    "        self.trans_flatten = nn.Flatten()\n",
    "        self.trans_linear = nn.Linear(150528, 2048)\n",
    "\n",
    "        # All the ResNet model\n",
    "        self.model_Res = model_Res\n",
    "\n",
    "        # Merge the result and pass \n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "        self.linear1 = nn.Linear(2048, 25)\n",
    "    \n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=2048, num_heads=8)\n",
    "     \n",
    "\n",
    "\n",
    "    def forward(self, trans_b, res_b):\n",
    "        # Get intermediate outputs using hidden layer\n",
    "        result_trans = self.model_trans_top(trans_b)\n",
    "        patch_state = result_trans.last_hidden_state[:,1:,:] # Remove the classification token and get the last hidden state of all patchs\n",
    "        #result_trans = self.trans_layer_norm(patch_state)\n",
    "        result_trans = self.trans_flatten(patch_state)\n",
    "        result_trans = self.dropout(result_trans)\n",
    "        result_trans = self.trans_linear(result_trans)\n",
    "\n",
    "       # print('\\n ViT features  shape', result_trans.shape)\n",
    "\n",
    "        result_res = self.model_Res(res_b)\n",
    "\n",
    "        # result_res = result_res.squeeze() # Batch size cannot be 1\n",
    "        result_res = torch.reshape(result_res, (result_res.shape[0], result_res.shape[1]))\n",
    "\n",
    "        x = result_trans.unsqueeze(0)\n",
    "        y = result_res.unsqueeze(0)\n",
    "        x, _ = self.self_attention(y,x,y)\n",
    "        result_merge = x.squeeze()\n",
    "        \n",
    "     \n",
    "\n",
    "        result_merge = self.dropout(result_merge)\n",
    "        result_merge = self.linear1(result_merge)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return result_merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T03:06:37.835596Z",
     "iopub.status.busy": "2023-11-03T03:06:37.834673Z",
     "iopub.status.idle": "2023-11-03T03:06:40.507483Z",
     "shell.execute_reply": "2023-11-03T03:06:40.506499Z",
     "shell.execute_reply.started": "2023-11-03T03:06:37.835566Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model_final(model_trans_top, model_Res, dp_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T03:06:41.987795Z",
     "iopub.status.busy": "2023-11-03T03:06:41.986997Z",
     "iopub.status.idle": "2023-11-03T03:06:47.669762Z",
     "shell.execute_reply": "2023-11-03T03:06:47.668743Z",
     "shell.execute_reply.started": "2023-11-03T03:06:41.987763Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl=DeviceDataLoader(val_dl, device)\n",
    "model = to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T03:06:52.079266Z",
     "iopub.status.busy": "2023-11-03T03:06:52.078807Z",
     "iopub.status.idle": "2023-11-03T03:06:52.098711Z",
     "shell.execute_reply": "2023-11-03T03:06:52.097744Z",
     "shell.execute_reply.started": "2023-11-03T03:06:52.079233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define optimizer and learning_rate scheduler\n",
    "params = [param for param in list(model.parameters()) if param.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.1, \n",
    "    patience=2, \n",
    "    verbose=True)\n",
    "\n",
    "def fit(epochs, model, train_dl, val_dl, patience=5):\n",
    "    device = next(model.parameters()).device\n",
    "    opt = optimizer\n",
    "    sched = lr_scheduler\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_num = 1\n",
    "        total_loss = 0\n",
    "        for x_trans, x_res, yb in train_dl:\n",
    "            x_trans = x_trans.to(device)\n",
    "            x_res = x_res.to(device)\n",
    "            yb = yb.to(device)\n",
    "            \n",
    "            preds = model(x_trans, x_res)\n",
    "            loss = loss_func(preds.squeeze(), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            print('\\r', f'batch #{batch_num}: {loss}', end='')\n",
    "            batch_num += 1\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        sched.step(total_loss)\n",
    "\n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        with torch.no_grad():\n",
    "            for x_trans, x_res, yb in val_dl:\n",
    "                x_trans = x_trans.to(device)\n",
    "                x_res = x_res.to(device)\n",
    "                yb = yb.to(device)\n",
    "                \n",
    "                preds = model(x_trans, x_res)\n",
    "                _, predicted = torch.max(preds.data, 1)\n",
    "                num_correct += (predicted == yb).sum().item()\n",
    "                num_total += yb.size(0)\n",
    "        \n",
    "        val_accuracy = 100 * num_correct / num_total\n",
    "        print('\\n', f'Epoch: ({epoch+1}/{epochs}) Loss = {total_loss}, Validation Accuracy = {val_accuracy}%')\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "        \n",
    "        if no_improvement_count >= patience:\n",
    "            print(f'\\nEarly stopping after {patience} epochs without improvement.')\n",
    "            break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T03:06:56.284827Z",
     "iopub.status.busy": "2023-11-03T03:06:56.284142Z",
     "iopub.status.idle": "2023-11-03T04:15:03.399645Z",
     "shell.execute_reply": "2023-11-03T04:15:03.398730Z",
     "shell.execute_reply.started": "2023-11-03T03:06:56.284796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch #157: 3.1004095077514656\n",
      " Epoch: (1/50) Loss = 506.90197944641113, Validation Accuracy = 5.12%\n",
      " batch #157: 3.1499962806701665\n",
      " Epoch: (2/50) Loss = 502.3433105945587, Validation Accuracy = 7.04%\n",
      " batch #157: 3.1510057449340822\n",
      " Epoch: (3/50) Loss = 498.84123373031616, Validation Accuracy = 8.64%\n",
      " batch #157: 3.0552377700805664\n",
      " Epoch: (4/50) Loss = 495.77550530433655, Validation Accuracy = 10.24%\n",
      " batch #157: 3.1232292652130127\n",
      " Epoch: (5/50) Loss = 493.42453145980835, Validation Accuracy = 9.28%\n",
      " batch #157: 3.2011275291442875\n",
      " Epoch: (6/50) Loss = 491.0006353855133, Validation Accuracy = 9.76%\n",
      " batch #157: 3.2438364028930664\n",
      " Epoch: (7/50) Loss = 489.94416189193726, Validation Accuracy = 11.84%\n",
      " batch #157: 3.2377071380615234\n",
      " Epoch: (8/50) Loss = 486.92253279685974, Validation Accuracy = 13.44%\n",
      " batch #157: 3.1728820800781252\n",
      " Epoch: (9/50) Loss = 485.3731861114502, Validation Accuracy = 13.6%\n",
      " batch #157: 3.2546203136444095\n",
      " Epoch: (10/50) Loss = 485.03262662887573, Validation Accuracy = 12.8%\n",
      " batch #157: 3.0327765941619873\n",
      " Epoch: (11/50) Loss = 483.29547452926636, Validation Accuracy = 13.92%\n",
      " batch #157: 2.8945209980010986\n",
      " Epoch: (12/50) Loss = 480.1765546798706, Validation Accuracy = 14.08%\n",
      " batch #157: 3.0075256824493418\n",
      " Epoch: (13/50) Loss = 479.3385615348816, Validation Accuracy = 15.36%\n",
      " batch #157: 3.0836653709411626\n",
      " Epoch: (14/50) Loss = 477.3198981285095, Validation Accuracy = 17.6%\n",
      " batch #157: 2.9105994701385546\n",
      " Epoch: (15/50) Loss = 476.36889910697937, Validation Accuracy = 15.52%\n",
      " batch #157: 3.1521272659301765\n",
      " Epoch: (16/50) Loss = 475.7625606060028, Validation Accuracy = 16.96%\n",
      " batch #157: 3.0687174797058105\n",
      " Epoch: (17/50) Loss = 473.43768644332886, Validation Accuracy = 16.16%\n",
      " batch #157: 2.8651480674743652\n",
      " Epoch: (18/50) Loss = 471.21846628189087, Validation Accuracy = 18.08%\n",
      " batch #157: 3.1146204471588135\n",
      " Epoch: (19/50) Loss = 469.7759892940521, Validation Accuracy = 18.56%\n",
      " batch #157: 3.1276688575744637\n",
      " Epoch: (20/50) Loss = 468.7745156288147, Validation Accuracy = 17.6%\n",
      " batch #157: 3.2577888965606698\n",
      " Epoch: (21/50) Loss = 466.24423837661743, Validation Accuracy = 21.28%\n",
      " batch #157: 3.0330791473388674\n",
      " Epoch: (22/50) Loss = 463.8200373649597, Validation Accuracy = 20.16%\n",
      " batch #157: 2.6863644123077393\n",
      " Epoch: (23/50) Loss = 461.8190186023712, Validation Accuracy = 19.68%\n",
      " batch #157: 2.9986557960510254\n",
      " Epoch: (24/50) Loss = 459.23332166671753, Validation Accuracy = 20.48%\n",
      " batch #157: 2.6769762039184574\n",
      " Epoch: (25/50) Loss = 455.39737915992737, Validation Accuracy = 21.44%\n",
      " batch #157: 3.1092391014099125\n",
      " Epoch: (26/50) Loss = 452.9481475353241, Validation Accuracy = 20.16%\n",
      " batch #157: 2.9746258258819584\n",
      " Epoch: (27/50) Loss = 448.23320055007935, Validation Accuracy = 21.44%\n",
      " batch #157: 2.8204026222229004\n",
      " Epoch: (28/50) Loss = 444.7937786579132, Validation Accuracy = 23.2%\n",
      " batch #157: 3.2084312438964844\n",
      " Epoch: (29/50) Loss = 439.58737874031067, Validation Accuracy = 24.48%\n",
      " batch #157: 2.8005759716033936\n",
      " Epoch: (30/50) Loss = 433.5311961174011, Validation Accuracy = 24.96%\n",
      " batch #157: 3.1776952743530273\n",
      " Epoch: (31/50) Loss = 426.38776564598083, Validation Accuracy = 25.76%\n",
      " batch #157: 2.4612095355987557\n",
      " Epoch: (32/50) Loss = 418.3454611301422, Validation Accuracy = 26.24%\n",
      " batch #157: 2.4713628292083743\n",
      " Epoch: (33/50) Loss = 408.44983410835266, Validation Accuracy = 28.0%\n",
      " batch #157: 2.5388731956481934\n",
      " Epoch: (34/50) Loss = 400.6210548877716, Validation Accuracy = 30.56%\n",
      " batch #157: 2.7548477649688724\n",
      " Epoch: (35/50) Loss = 390.07947993278503, Validation Accuracy = 32.96%\n",
      " batch #157: 2.3212156295776367\n",
      " Epoch: (36/50) Loss = 380.55646300315857, Validation Accuracy = 33.6%\n",
      " batch #157: 2.1371939182281494\n",
      " Epoch: (37/50) Loss = 373.4019207954407, Validation Accuracy = 35.04%\n",
      " batch #157: 2.4653375148773193\n",
      " Epoch: (38/50) Loss = 368.3869309425354, Validation Accuracy = 33.28%\n",
      " batch #157: 2.5084054470062256\n",
      " Epoch: (39/50) Loss = 362.5055898427963, Validation Accuracy = 33.76%\n",
      " batch #157: 2.8576779365539558\n",
      " Epoch: (40/50) Loss = 353.2794466018677, Validation Accuracy = 37.76%\n",
      " batch #157: 2.5092582702636727\n",
      " Epoch: (41/50) Loss = 346.70978021621704, Validation Accuracy = 37.6%\n",
      " batch #157: 1.9453533887863164\n",
      " Epoch: (42/50) Loss = 344.7249456644058, Validation Accuracy = 36.96%\n",
      " batch #157: 2.3472845554351807\n",
      " Epoch: (43/50) Loss = 339.02074682712555, Validation Accuracy = 38.08%\n",
      " batch #157: 2.2926723957061768\n",
      " Epoch: (44/50) Loss = 337.5694090127945, Validation Accuracy = 39.04%\n",
      " batch #157: 2.0418722629547124\n",
      " Epoch: (45/50) Loss = 330.45457315444946, Validation Accuracy = 40.16%\n",
      " batch #157: 1.9293211698532104\n",
      " Epoch: (46/50) Loss = 323.1808451414108, Validation Accuracy = 40.64%\n",
      " batch #157: 2.5068526268005376\n",
      " Epoch: (47/50) Loss = 320.2451595067978, Validation Accuracy = 41.76%\n",
      " batch #157: 1.7475326061248782\n",
      " Epoch: (48/50) Loss = 316.2370344400406, Validation Accuracy = 43.04%\n",
      " batch #157: 2.3437747955322266\n",
      " Epoch: (49/50) Loss = 310.96476459503174, Validation Accuracy = 44.0%\n",
      " batch #157: 1.7987302541732788\n",
      " Epoch: (50/50) Loss = 302.51730275154114, Validation Accuracy = 43.2%\n"
     ]
    }
   ],
   "source": [
    "fit(50, model, train_dl, val_dl,10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
